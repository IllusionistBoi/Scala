package com.sundogsoftware.spark

import org.apache.log4j._
import org.apache.spark._

/** Count up how many of each star rating exists in the MovieLens 100K data set. */
object RatingsCounter {

  /** Our main function where the action happens */
  def main(args: Array[String]) {

    // Set the log level to only print errors
    Logger.getLogger("org").setLevel(Level.ERROR)

    // This creates a SparkContext that uses every core of the local machine with name RatingsCounter.
    val sc = new SparkContext("local[*]", "RatingsCounter1")

    // This command is used to load up each line of the ratings data in an Resilient Distributed Datasets (RDD)
    val lines = sc.textFile("data/ml-100k/u.data")

    // This command is used to convert each line into a string, followed by splitting it out by tabs and then finally, extract the third field.
    // (The file format is userID, movieID, rating, timestamp)
    val ratings = lines.map(x => x.toString().split("\t")(2))

    // This command is used to count the number of times each value (rating) occurs 
    val results = ratings.countByValue()

    //  This command is used to sort the resulting map of (rating, count) tuples
    val sortedResults = results.toSeq.sortBy(_._1)

    // Print each result on its own line.
    sortedResults.foreach(println)

    sc.stop()

    println("The output for the quiz is above!")
  }

}
